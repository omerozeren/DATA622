---
title: "DATA 622 - Homework 2"
author: "OMER OZEREN"
output:
  html_document:
    highlight: tango
    theme: journal
    toc: yes
    toc_depth: 5
    toc_float: yes
---

```{r echo=FALSE, include=FALSE}
library(class)
library(pROC)
library(ROCR)
library(caret)
library(C50)
library(e1071)
library(naivebayes)
library(MASS)
library(mltools)
library(knitr)
```

## A)

Run Bagging (ipred package)   

* sample with replacement

* estimate metrics for a model

* repeat as many times as specied and report the average



### Load the Data

```{r}
df <- read.table("C:/Users/OMERO/Documents/GitHub/DATA622/data.txt",header = T,sep=',')
df$label <- ifelse(df$label =="BLACK",1,0)
df$y <- as.numeric(df$y)
df$X <- as.factor(df$X)
```

### Split Data into Train (70%) and Test data(30%)  

```{r}
set.seed(42)
split_df <- createDataPartition(df$label, p = .70, list = FALSE)
df_train <- df[split_df,]
df_test <- df[-split_df,]
```

### Model Performance Estimator
```{r}
estimate_model_performance <- function(y_true, y_pred, model_name){
  cm <- confusionMatrix(table(y_true, y_pred))
  cm_table <- cm$table
  tpr <- cm_table[[1]] / (cm_table[[1]] + cm_table[[4]])
  fnr <- 1 - tpr
  fpr <- cm_table[[3]] / (cm_table[[3]] + cm_table[[4]])
  tnr <- 1 - fpr
  accuracy <- cm$overall[[1]]
  for_auc <- prediction(c(y_pred), y_true)
  auc <- performance(for_auc, "auc")
  auc <- auc@y.values[[1]]
  return(data.frame(Algo = model_name, AUC = auc, ACCURACY = accuracy, TPR = tpr, FPR = fpr, TNR = tnr, FNR = fnr))
}
```


### NB Model Building

```{r}
nb_model<-naiveBayes(df_train$label~.,data=df_train)
nb_testpred<-predict(nb_model,df_test,type='raw')
nb_testclass<-unlist(apply(round(nb_testpred),1,which.max))-1
nb_table<-table(df_test$label, nb_testclass)
nb_cm<-caret::confusionMatrix(nb_table)
```

### Estimate NB model test data () performance

```{r}
estimate_model_performance(df_test$label,nb_testclass,'NB')
```


### Bagging MEthodology - NB Model

```{r}
# iterate the model 200 times
runModel<-function(df_train) {naiveBayes(df_train$label~.,data=df_train[sample(1:nrow(df_train),nrow(df_train),replace=T),])}
lapplyrunmodel<-function(x)runModel(df_train)
models<-lapply(1:200,lapplyrunmodel)
bagging_preds<-lapply(models,FUN=function(M,D=df_test)predict(M,D,type='raw'))
bagging_cfm<-lapply(bagging_preds,FUN=function(P,A=df_test$label)
{pred_class<-unlist(apply(round(P),1,which.max))-1
  pred_tbl<-table(A,pred_class)
  pred_cfm<-caret::confusionMatrix(pred_tbl)
  pred_cfm
})
bagging.perf<-as.data.frame(do.call('rbind',lapply(bagging_cfm,FUN=function(cfm)c(cfm$overall,cfm$byClass))))
bagging.perf.mean<-mean(bagging.perf$Accuracy)
bagging.perf.var<-sd(bagging.perf$Accuracy)
```

### Variance of Bagging Methodology - 200 times NB model run 
```{r  echo=FALSE}
bagging.perf.var
```

### Mean of Bagging Methodology - 200 times NB model run 
```{r  echo=FALSE}
bagging.perf.mean
```

### Logistic Regression

```{r}
logit_model <- glm(df_train$label ~ ., data = df_train, family = "binomial")
logit_prob <- predict(logit_model, df_test, type = "response")
logit_y_pred <- as.factor(ifelse(logit_prob <= 0.5, 0, 1))
```

### Estimate Logistic model test data () performance

```{r}
estimate_model_performance(df_test$label,logit_y_pred,'Logistic')
```

### Bagging MEthodology - Logistic Model

```{r}
# iterate the model 200 times
runModel<-function(df_train) {glm(df_train$label~.,data=df_train[sample(1:nrow(df_train),nrow(df_train),replace=T),],family = "binomial")}
lapplyrunmodel<-function(x)runModel(df_train)
models<-lapply(1:200,lapplyrunmodel)
bagging_preds<-lapply(models,FUN=function(M,D=df_test)predict(M,D,type='response'))
bagging_cfm<-lapply(bagging_preds,FUN=function(P,A=df_test$label)
{pred_class<-unlist(apply(round(P),1,which.max))-1
  pred_tbl<-table(A,pred_class)
  pred_cfm<-caret::confusionMatrix(pred_tbl)
  pred_cfm
})
bagging.perf<-as.data.frame(do.call('rbind',lapply(bagging_cfm,FUN=function(cfm)c(cfm$overall,cfm$byClass))))
bagging.perf.mean<-mean(bagging.perf$Accuracy)
bagging.perf.var<-sd(bagging.perf$Accuracy)
```

